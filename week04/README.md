### Lecture 4: Deep Learning 101
In this lecture, we will talk about the basics of DL.  
The definition: how is it different from AI and ML? Artificial Neurons, Neural Layers. Feed Forward networks. Multi-layer Perceptron. Back propagation. Normalization. Regularization. For the practical task, we will study basics of NLP: tokenization, evaluation metrics, architecture of transformers. 

#### Reading:
-  A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay, Leslie N. Smith  
-  Bag of Tricks for Image Classification with Convolutional Neural Networks, Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, Mu Li  
-  The [1Cycle Policy](https://sgugger.github.io/the-1cycle-policy.html), Sylvain Gugger  
- [Using Transformers](https://huggingface.co/course/chapter2/1?fw=pt)
-  Pytorch [documentation](https://pytorch.org/docs/stable/index.html ) 
-  [Jigsaw toxicity dataset (for the homework)](https://huggingface.co/datasets/jigsaw_toxicity_pred)
