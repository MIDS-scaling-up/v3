{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_SRGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b0cedad8e44a47e6892f3a1bf301e816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4fbc481bc81d4555a5e3399e8d249bc1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e68e3c06507046f5b4619ac013d78678",
              "IPY_MODEL_4fa7333022644bcaaf6e77328131e22e",
              "IPY_MODEL_75c3ce41d15a4df6bb3f890c34fcdf98"
            ]
          }
        },
        "4fbc481bc81d4555a5e3399e8d249bc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "e68e3c06507046f5b4619ac013d78678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_31374dbd67594b80aa07350b608ec178",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Epoch 0:  39%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aed37245d6ec4a1c9962cec08402e6bb"
          }
        },
        "4fa7333022644bcaaf6e77328131e22e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0023863bd14142ee85bf630fdee345ac",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 3437,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1340,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3d2d9d9c5cad43f4931ea6d140df7614"
          }
        },
        "75c3ce41d15a4df6bb3f890c34fcdf98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_43dfede5c12349e19546338f958ab322",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1340/3437 [04:59&lt;07:49,  4.47it/s, loss=0.0877, v_num=1]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5dbf797ccb504a41892d7847c9201ea9"
          }
        },
        "31374dbd67594b80aa07350b608ec178": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aed37245d6ec4a1c9962cec08402e6bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0023863bd14142ee85bf630fdee345ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3d2d9d9c5cad43f4931ea6d140df7614": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "43dfede5c12349e19546338f958ab322": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5dbf797ccb504a41892d7847c9201ea9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install  --quiet \"lightning-bolts\" \"torchvision\" \"torchmetrics\""
      ],
      "metadata": {
        "id": "PJjeB2kvKazR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c51d0863-5f54-4ad4-e1e1-7f16537eeee5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 316 kB 3.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 397 kB 11.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 527 kB 13.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 20.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 952 kB 24.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 32.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 7.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 46.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 45.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 48.4 MB/s \n",
            "\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import ArgumentParser\n",
        "from pathlib import Path\n",
        "from typing import Any, List, Optional, Tuple\n",
        "from warnings import warn\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import vgg19\n",
        "\n",
        "from pl_bolts.callbacks import SRImageLoggerCallback\n",
        "from pl_bolts.datamodules import TVTDataModule\n",
        "from pl_bolts.datasets.utils import prepare_sr_datasets\n",
        "\n",
        "from pl_bolts.models.gans import SRGAN\n",
        "from pl_bolts.models.gans.srgan.components import SRGANDiscriminator, SRGANGenerator, VGG19FeatureExtractor"
      ],
      "metadata": {
        "id": "BNIfbSvR5Poc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Super-resolution GAN (SRGAN)\n",
        "Credit: https://github.com/https-deeplearning-ai/GANs-Public  \n",
        "\n",
        "*Please note that this is meant to introduce more advanced concepts. If you’re up for a challenge, take a look and don’t worry if you can’t follow everything. There is no code to implement—only some cool code for you to learn and run!*\n",
        "\n",
        "It is recommended that you should already be familiar with:\n",
        " - Residual blocks, from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) (He et al. 2015)\n",
        " - Perceptual loss, from [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155) (Johnson et al. 2016)\n",
        " - VGG architecture, from [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556) (Simonyan et al. 2015)\n",
        "\n",
        "### Goals\n",
        "\n",
        "In this notebook, you will learn about Super-Resolution GAN (SRGAN), a GAN that enhances the resolution of images by 4x, proposed in [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/abs/1609.04802) (Ledig et al. 2017). You will also implement the architecture and training in full and be able to train it on the CIFAR dataset.\n",
        "\n",
        "### Background\n",
        "\n",
        "The authors first train a super-resolution residual network (SRResNet) with standard pixel-wise loss that achieves state-of-the-art metrics. They then insert this as the generator in the SRGAN framework, which is trained with a combination of pixel-wise, perceptual, and adversarial losses."
      ],
      "metadata": {
        "id": "iY7hnqhcFwkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SRGAN Submodules\n",
        "\n",
        "Before jumping into SRGAN, let's first take a look at some components that will be useful later.  "
      ],
      "metadata": {
        "id": "B9A1KZ7QGUdy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parametric ReLU (PReLU)\n",
        "\n",
        "As you already know, ReLU is one of the simplest activation functions that can be described as\n",
        "\n",
        "\\begin{align*}\n",
        "    x_{\\text{ReLU}} := \\max(0, x),\n",
        "\\end{align*}\n",
        "\n",
        "where negative values of $x$ become thresholded at $0$. However, this stops gradient through these negative values, which can hinder training. The authors of [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852) addressed this by introducing a more general ReLU by scaling negative values by some constant $a > 0$:\n",
        "\n",
        "\\begin{align*}\n",
        "    x_{\\text{PReLU}} := \\max(0, x) + a * \\min(0, x).\n",
        "\\end{align*}\n",
        "\n",
        "Conveniently, this is implemented in Pytorch as [torch.nn.PReLU](https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html)"
      ],
      "metadata": {
        "id": "FZ7EGH0rF-9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Residual Blocks\n",
        "\n",
        "The residual block, which is relevant in many state-of-the-art computer vision models, is used in all parts of SRGAN and is similar to the ones used in Pix2PixHD (see optional notebook). If you're not familiar with residual blocks, please take a look [here](https://paperswithcode.com/method/residual-block). Now, you'll start by first implementing a basic residual block."
      ],
      "metadata": {
        "id": "HSpTVx1CGcHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, feature_maps: int = 64) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(feature_maps, feature_maps, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(feature_maps),\n",
        "            nn.PReLU(),\n",
        "            nn.Conv2d(feature_maps, feature_maps, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(feature_maps),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return x + self.block(x)"
      ],
      "metadata": {
        "id": "zH3QSIrNGYIf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  PixelShuffle\n",
        "\n",
        "Proposed in [Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network](https://arxiv.org/abs/1609.05158) (Shi et al. 2016), PixelShuffle, also called sub-pixel convolution, is another way to upsample an image.\n",
        "\n",
        "PixelShuffle simply reshapes a $r^2C\\ x\\ H\\ x\\ W$ tensor into a $C\\ x\\ rH\\ x\\ rW$ tensor, essentially trading channel information for spatial information. Instead of convolving with stride $1/r$ as in deconvolution, the authors think about the weights in the kernel as being spaced $1/r$ pixels apart. When sliding this kernel over an input, the weights that fall between pixels aren't activated and don't need need to be calculated. The total number of activation patterns is thus increased by a factor of $r^2$. This operation is illustrated in the figure below.\n",
        "\n",
        "Don't worry if this is confusing! The algorithm is conveniently implemented as `torch.nn.PixelShuffle` in PyTorch, so as long as you have a general idea of how this works, you're set.\n",
        "\n",
        "> ![Efficient Sub-pixel CNN](https://github.com/https-deeplearning-ai/GANs-Public/blob/master/SRGAN-PixelShuffle.png?raw=true)\n",
        "*Efficient sub-pixel CNN, taken from Figure 1 of [Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network](https://arxiv.org/abs/1609.05158) (Shi et al. 2016). The PixelShuffle operation (also known as sub-pixel convolution) is shown as the last step on the right.*"
      ],
      "metadata": {
        "id": "aaxYGBWoGv6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SRGAN Parts\n",
        "\n",
        "Now that you've learned about the various SRGAN submodules, you can now use them to build the generator and discriminator!"
      ],
      "metadata": {
        "id": "49fg8O_nHbi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generator (SRResNet)\n",
        "\n",
        "The super-resolution residual network (SRResNet) and the generator are the same thing. The generator network architecture is actually quite simple - just a bunch of convolutional layers, residual blocks, and pixel shuffling layers!\n",
        "\n",
        "> ![SRGAN Generator](https://github.com/https-deeplearning-ai/GANs-Public/blob/master/SRGAN-Generator.png?raw=true)\n",
        "*SRGAN Generator, taken from Figure 4 of [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/abs/1609.04802) (Ledig et al. 2017).*"
      ],
      "metadata": {
        "id": "ycu82ebpHg9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SRGANGenerator(nn.Module):\n",
        "    '''\n",
        "    Generator Class\n",
        "    Values:\n",
        "        image_channels: number of channels throughout the generator, a scalar\n",
        "        num_ps_blocks: number of PixelShuffle blocks, a scalar\n",
        "        num_res_blocks: number of residual blocks, a scalar\n",
        "    '''  \n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_channels: int,\n",
        "        feature_maps: int = 64,\n",
        "        num_res_blocks: int = 16,\n",
        "        num_ps_blocks: int = 2,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        # Input block \n",
        "        self.input_block = nn.Sequential(\n",
        "            nn.Conv2d(image_channels, feature_maps, kernel_size=9, padding=4),\n",
        "            nn.PReLU(),\n",
        "        )\n",
        "\n",
        "        # B residual blocks \n",
        "        res_blocks = []\n",
        "        for _ in range(num_res_blocks):\n",
        "            res_blocks += [ResidualBlock(feature_maps)]\n",
        "\n",
        "        # k3n64s1\n",
        "        res_blocks += [\n",
        "            nn.Conv2d(feature_maps, feature_maps, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(feature_maps),\n",
        "        ]\n",
        "        self.res_blocks = nn.Sequential(*res_blocks)\n",
        "\n",
        "        # PixelShuffle blocks\n",
        "        ps_blocks = []\n",
        "        for _ in range(num_ps_blocks):\n",
        "            ps_blocks += [\n",
        "                nn.Conv2d(feature_maps, 4 * feature_maps, kernel_size=3, padding=1),\n",
        "                nn.PixelShuffle(2),\n",
        "                nn.PReLU(),\n",
        "            ]\n",
        "        self.ps_blocks = nn.Sequential(*ps_blocks)\n",
        "\n",
        "        # Output block \n",
        "        self.output_block = nn.Sequential(\n",
        "            nn.Conv2d(feature_maps, image_channels, kernel_size=9, padding=4),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x_res = self.input_block(x)\n",
        "        x = x_res + self.res_blocks(x_res)\n",
        "        x = self.ps_blocks(x)\n",
        "        x = self.output_block(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "FPM3M0VM8FN0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discriminator\n",
        "\n",
        "The discriminator architecture is also relatively straightforward, just one big sequential model - see the diagram below for reference!\n",
        "\n",
        "![SRGAN Generator](https://github.com/https-deeplearning-ai/GANs-Public/blob/master/SRGAN-Discriminator.png?raw=true)\n",
        "*SRGAN Discriminator, taken from Figure 4 of [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/abs/1609.04802) (Ledig et al. 2017).*"
      ],
      "metadata": {
        "id": "aFxjUvS78Q-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SRGANDiscriminator(nn.Module):\n",
        "    def __init__(self, image_channels: int, feature_maps: int = 64) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            self._make_double_conv_block(image_channels, feature_maps, first_batch_norm=False),\n",
        "            self._make_double_conv_block(feature_maps, feature_maps * 2),\n",
        "            self._make_double_conv_block(feature_maps * 2, feature_maps * 4),\n",
        "            self._make_double_conv_block(feature_maps * 4, feature_maps * 8),\n",
        "        )\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(feature_maps * 8, feature_maps * 16, kernel_size=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(feature_maps * 16, 1, kernel_size=1),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "    def _make_double_conv_block(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        first_batch_norm: bool = True,\n",
        "    ) -> nn.Sequential:\n",
        "        return nn.Sequential(\n",
        "            self._make_conv_block(in_channels, out_channels, batch_norm=first_batch_norm),\n",
        "            self._make_conv_block(out_channels, out_channels, stride=2),\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_conv_block(\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        stride: int = 1,\n",
        "        batch_norm: bool = True,\n",
        "    ) -> nn.Sequential:\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n",
        "            nn.BatchNorm2d(out_channels) if batch_norm else nn.Identity(),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv_blocks(x)\n",
        "        x = self.mlp(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "vFg5E2kI8QAX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG19FeatureExtractor(nn.Module):\n",
        "    def __init__(self, image_channels: int = 3) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        assert image_channels in [1, 3]\n",
        "        self.image_channels = image_channels\n",
        "\n",
        "        vgg = vgg19(pretrained=True)\n",
        "        self.vgg = nn.Sequential(*list(vgg.features)[:-1]).eval()\n",
        "        for p in self.vgg.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.image_channels == 1:\n",
        "            x = x.repeat(1, 3, 1, 1)\n",
        "\n",
        "        return self.vgg(x)"
      ],
      "metadata": {
        "id": "RQPtmqCI8vpx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C. SRGAN"
      ],
      "metadata": {
        "id": "ocOdxwea-7tY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SRGAN(pl.LightningModule):\n",
        "  def __init__(\n",
        "      self,\n",
        "      image_channels: int = 3,\n",
        "      feature_maps_gen: int = 64,\n",
        "      feature_maps_disc: int = 64,\n",
        "      num_res_blocks: int = 16,\n",
        "      scale_factor: int = 4,\n",
        "      generator_checkpoint: Optional[str] = None,\n",
        "      learning_rate: float = 1e-4,\n",
        "      scheduler_step: int = 100,\n",
        "      **kwargs: Any,\n",
        "  ) -> None:\n",
        "      \"\"\"\n",
        "      Args:\n",
        "          image_channels: Number of channels of the images from the dataset\n",
        "          feature_maps_gen: Number of feature maps to use for the generator\n",
        "          feature_maps_disc: Number of feature maps to use for the discriminator\n",
        "          num_res_blocks: Number of res blocks to use in the generator\n",
        "          scale_factor: Scale factor for the images (either 2 or 4)\n",
        "          generator_checkpoint: Generator checkpoint created with SRResNet module\n",
        "          learning_rate: Learning rate\n",
        "          scheduler_step: Number of epochs after which the learning rate gets decayed\n",
        "      \"\"\"\n",
        "      super().__init__()\n",
        "      self.save_hyperparameters()\n",
        "\n",
        "      if generator_checkpoint:\n",
        "          self.generator = torch.load(generator_checkpoint)\n",
        "      else:\n",
        "          assert scale_factor in [2, 4]\n",
        "          num_ps_blocks = scale_factor // 2\n",
        "          self.generator = SRGANGenerator(image_channels, feature_maps_gen, num_res_blocks, num_ps_blocks)\n",
        "\n",
        "      self.discriminator = SRGANDiscriminator(image_channels, feature_maps_disc)\n",
        "      self.vgg_feature_extractor = VGG19FeatureExtractor(image_channels)\n",
        "\n",
        "  def configure_optimizers(self) -> Tuple[List[torch.optim.Adam], List[torch.optim.lr_scheduler.MultiStepLR]]:\n",
        "      opt_disc = torch.optim.Adam(self.discriminator.parameters(), lr=self.hparams.learning_rate)\n",
        "      opt_gen = torch.optim.Adam(self.generator.parameters(), lr=self.hparams.learning_rate)\n",
        "\n",
        "      sched_disc = torch.optim.lr_scheduler.MultiStepLR(opt_disc, milestones=[self.hparams.scheduler_step], gamma=0.1)\n",
        "      sched_gen = torch.optim.lr_scheduler.MultiStepLR(opt_gen, milestones=[self.hparams.scheduler_step], gamma=0.1)\n",
        "      return [opt_disc, opt_gen], [sched_disc, sched_gen]\n",
        "\n",
        "  def forward(self, lr_image: torch.Tensor) -> torch.Tensor:\n",
        "      \"\"\"Generates a high resolution image given a low resolution image.\n",
        "      Example::\n",
        "          srgan = SRGAN.load_from_checkpoint(PATH)\n",
        "          hr_image = srgan(lr_image)\n",
        "      \"\"\"\n",
        "      return self.generator(lr_image)\n",
        "    \n",
        "  def training_step(\n",
        "      self,\n",
        "      batch: Tuple[torch.Tensor, torch.Tensor],\n",
        "      batch_idx: int,\n",
        "      optimizer_idx: int,\n",
        "  ) -> torch.Tensor:\n",
        "      hr_image, lr_image = batch\n",
        "\n",
        "      # Train discriminator\n",
        "      result = None\n",
        "      if optimizer_idx == 0:\n",
        "          result = self._disc_step(hr_image, lr_image)\n",
        "\n",
        "      # Train generator\n",
        "      if optimizer_idx == 1:\n",
        "          result = self._gen_step(hr_image, lr_image)\n",
        "\n",
        "      return result\n",
        "\n",
        "  def _disc_step(self, hr_image: torch.Tensor, lr_image: torch.Tensor) -> torch.Tensor:\n",
        "      disc_loss = self._disc_loss(hr_image, lr_image)\n",
        "      self.log(\"loss/disc\", disc_loss, on_step=True, on_epoch=True)\n",
        "      return disc_loss\n",
        "\n",
        "  def _gen_step(self, hr_image: torch.Tensor, lr_image: torch.Tensor) -> torch.Tensor:\n",
        "      gen_loss = self._gen_loss(hr_image, lr_image)\n",
        "      self.log(\"loss/gen\", gen_loss, on_step=True, on_epoch=True)\n",
        "      return gen_loss\n",
        "\n",
        "  def _disc_loss(self, hr_image: torch.Tensor, lr_image: torch.Tensor) -> torch.Tensor:\n",
        "      real_pred = self.discriminator(hr_image)\n",
        "      real_loss = self._adv_loss(real_pred, ones=True)\n",
        "\n",
        "      _, fake_pred = self._fake_pred(lr_image)\n",
        "      fake_loss = self._adv_loss(fake_pred, ones=False)\n",
        "\n",
        "      disc_loss = 0.5 * (real_loss + fake_loss)\n",
        "\n",
        "      return disc_loss\n",
        "\n",
        "  def _gen_loss(self, hr_image: torch.Tensor, lr_image: torch.Tensor) -> torch.Tensor:\n",
        "      fake, fake_pred = self._fake_pred(lr_image)\n",
        "\n",
        "      perceptual_loss = self._perceptual_loss(hr_image, fake)\n",
        "      adv_loss = self._adv_loss(fake_pred, ones=True)\n",
        "      content_loss = self._content_loss(hr_image, fake)\n",
        "\n",
        "      gen_loss = 0.006 * perceptual_loss + 0.001 * adv_loss + content_loss\n",
        "\n",
        "      return gen_loss\n",
        "\n",
        "  def _fake_pred(self, lr_image: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "      fake = self(lr_image)\n",
        "      fake_pred = self.discriminator(fake)\n",
        "      return fake, fake_pred\n",
        "\n",
        "  @staticmethod\n",
        "  def _adv_loss(pred: torch.Tensor, ones: bool) -> torch.Tensor:\n",
        "      target = torch.ones_like(pred) if ones else torch.zeros_like(pred)\n",
        "      adv_loss = F.binary_cross_entropy_with_logits(pred, target)\n",
        "      return adv_loss\n",
        "\n",
        "  def _perceptual_loss(self, hr_image: torch.Tensor, fake: torch.Tensor) -> torch.Tensor:\n",
        "      real_features = self.vgg_feature_extractor(hr_image)\n",
        "      fake_features = self.vgg_feature_extractor(fake)\n",
        "      perceptual_loss = self._content_loss(real_features, fake_features)\n",
        "      return perceptual_loss\n",
        "\n",
        "  @staticmethod\n",
        "  def _content_loss(hr_image: torch.Tensor, fake: torch.Tensor) -> torch.Tensor:\n",
        "      return F.mse_loss(hr_image, fake)"
      ],
      "metadata": {
        "id": "NEbkzsKK9-NK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters\n",
        "* image_channels (int) – Number of channels of the images from the dataset\n",
        "\n",
        "* feature_maps_gen (int) – Number of feature maps to use for the generator\n",
        "\n",
        "* feature_maps_disc (int) – Number of feature maps to use for the discriminator\n",
        "\n",
        "* num_res_blocks (int) – Number of res blocks to use in the generator\n",
        "\n",
        "* scale_factor (int) – Scale factor for the images (either 2 or 4)\n",
        "\n",
        "* generator_checkpoint (Optional[str]) – Generator checkpoint created with SRResNet module\n",
        "\n",
        "* learning_rate (float) – Learning rate\n",
        "\n",
        "* scheduler_step (int) – Number of epochs after which the learning rate gets decayed"
      ],
      "metadata": {
        "id": "4oztUXiBLlv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets=[\"celeba\", \"mnist\", \"stl10\"]\n",
        "\n",
        "DATASET=\"mnist\"\n",
        "DATA_DIR=\"./\"\n",
        "AVAIL_GPUS=1\n",
        "\n",
        "IMAGE_CHANNELS=3\n",
        "FEATURE_MAPS_GEN=64\n",
        "FEATURE_MAPS_GEN=64\n",
        "NUM_RES_BLOCKS=16\n",
        "SCALE_FACTOR=4\n",
        "LEARNING_RATE=1e-4\n",
        "SCHEDULER_STEP=100"
      ],
      "metadata": {
        "id": "4oDEOBjmS-7m"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GENERATOR_CHECKPOINT = Path(f\"./srgan-{DATASET}-scale_factor={SCALE_FACTOR}.pt\")\n",
        "if not GENERATOR_CHECKPOINT.exists():\n",
        "    warn(\n",
        "        \"No generator checkpoint found. Training generator from scratch.\"\n",
        "    )\n",
        "    GENERATOR_CHECKPOINT = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMQsW0pGU1x-",
        "outputId": "2aaa061f-3d7c-4a1a-be4f-edf500b3e64d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: No generator checkpoint found. Training generator from scratch.\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "datasets = prepare_sr_datasets(DATASET, SCALE_FACTOR, DATA_DIR)\n",
        "dm = TVTDataModule(*datasets)\n",
        "model = SRGAN(generator_checkpoint=GENERATOR_CHECKPOINT,\n",
        "              image_channels=dm.dataset_test.image_channels,\n",
        "              feature_maps_gen=64,\n",
        "              feature_maps_disc=64,\n",
        "              num_res_blocks=NUM_RES_BLOCKS,\n",
        "              scale_factor=SCALE_FACTOR,\n",
        "              learning_rate=LEARNING_RATE,\n",
        "              scheduler_step=SCALE_FACTOR)\n",
        "trainer = Trainer(gpus=AVAIL_GPUS)\n",
        "trainer.fit(model, dm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462,
          "referenced_widgets": [
            "b0cedad8e44a47e6892f3a1bf301e816",
            "4fbc481bc81d4555a5e3399e8d249bc1",
            "e68e3c06507046f5b4619ac013d78678",
            "4fa7333022644bcaaf6e77328131e22e",
            "75c3ce41d15a4df6bb3f890c34fcdf98",
            "31374dbd67594b80aa07350b608ec178",
            "aed37245d6ec4a1c9962cec08402e6bb",
            "0023863bd14142ee85bf630fdee345ac",
            "3d2d9d9c5cad43f4931ea6d140df7614",
            "43dfede5c12349e19546338f958ab322",
            "5dbf797ccb504a41892d7847c9201ea9"
          ]
        },
        "id": "QGSelJleKN4U",
        "outputId": "79ee55cb-7316-41d9-f730-f9f39347ee9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py:120: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
            "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name                  | Type                  | Params\n",
            "----------------------------------------------------------------\n",
            "0 | generator             | SRGANGenerator        | 1.5 M \n",
            "1 | discriminator         | SRGANDiscriminator    | 5.2 M \n",
            "2 | vgg_feature_extractor | VGG19FeatureExtractor | 20.0 M\n",
            "----------------------------------------------------------------\n",
            "6.7 M     Trainable params\n",
            "20.0 M    Non-trainable params\n",
            "26.8 M    Total params\n",
            "107.070   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0cedad8e44a47e6892f3a1bf301e816",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr_image=\"LOW_RES_IMAGE_PATH\"\n",
        "srgan = SRGAN.load_from_checkpoint(PATH)\n",
        "hr_image = srgan(lr_image)"
      ],
      "metadata": {
        "id": "N0E2ox7ZPtc1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}